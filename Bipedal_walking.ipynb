{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment Setup - PyBullet Installation & Simulation Script\n",
    "Include code to install and import PyBullet, load the humanoid URDF on a plane, and run a basic simulation loop to verify dynamics and model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyBullet\n",
    "%pip install pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyBullet\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "\n",
    "# Connect to PyBullet in GUI mode\n",
    "p.connect(p.GUI)\n",
    "\n",
    "# Set the search path for URDF files\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# Load the plane URDF\n",
    "plane_id = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# Load the humanoid URDF\n",
    "humanoid_id = p.loadURDF(\"humanoid.urdf\", [0, 0, 1])  # Position the humanoid above the plane\n",
    "\n",
    "# Set gravity for the simulation\n",
    "p.setGravity(0, 0, -9.8)\n",
    "\n",
    "# Run a basic simulation loop\n",
    "for _ in range(1000):  # Run for 1000 simulation steps\n",
    "    p.stepSimulation()  # Step the simulation\n",
    "    time.sleep(1.0 / 240.0)  # Sleep to match real-time simulation speed\n",
    "\n",
    "# Disconnect from PyBullet\n",
    "p.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Gym Environment Development - Environment Creation & Integration\n",
    "Develop a Gym environment by subclassing gym.Env and implementing reset(), step(action), and optionally render(). Integrate the PyBullet simulation to define observation and action spaces along with a reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class HumanoidBipedalEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(HumanoidBipedalEnv, self).__init__()\n",
    "        \n",
    "        # Connect to PyBullet in DIRECT mode for headless simulation\n",
    "        self.client = p.connect(p.DIRECT)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        \n",
    "        # Load the plane and humanoid URDFs\n",
    "        self.plane_id = p.loadURDF(\"plane.urdf\")\n",
    "        self.humanoid_id = p.loadURDF(\"humanoid.urdf\", [0, 0, 1])\n",
    "        \n",
    "        # Set gravity\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        \n",
    "        # Define observation space (e.g., joint angles, velocities)\n",
    "        self.num_joints = p.getNumJoints(self.humanoid_id)\n",
    "        obs_high = np.array([np.pi] * self.num_joints + [np.inf] * self.num_joints, dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(-obs_high, obs_high, dtype=np.float32)\n",
    "        \n",
    "        # Define action space (e.g., joint torques)\n",
    "        action_high = np.array([1.0] * self.num_joints, dtype=np.float32)\n",
    "        self.action_space = spaces.Box(-action_high, action_high, dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the simulation\n",
    "        p.resetSimulation(self.client)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        self.plane_id = p.loadURDF(\"plane.urdf\")\n",
    "        self.humanoid_id = p.loadURDF(\"humanoid.urdf\", [0, 0, 1])\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        \n",
    "        # Return the initial observation\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply actions to the humanoid joints\n",
    "        for joint_index in range(self.num_joints):\n",
    "            p.setJointMotorControl2(\n",
    "                bodyUniqueId=self.humanoid_id,\n",
    "                jointIndex=joint_index,\n",
    "                controlMode=p.TORQUE_CONTROL,\n",
    "                force=action[joint_index]\n",
    "            )\n",
    "        \n",
    "        # Step the simulation\n",
    "        p.stepSimulation()\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward()\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        done = self._check_done()\n",
    "        \n",
    "        # Get the next observation\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        # Optional: Implement rendering logic if needed\n",
    "        pass\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        # Get joint states (angles and velocities)\n",
    "        joint_states = p.getJointStates(self.humanoid_id, range(self.num_joints))\n",
    "        joint_angles = np.array([state[0] for state in joint_states], dtype=np.float32)\n",
    "        joint_velocities = np.array([state[1] for state in joint_states], dtype=np.float32)\n",
    "        return np.concatenate([joint_angles, joint_velocities])\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        # Example reward function: forward movement and stability\n",
    "        base_position, _ = p.getBasePositionAndOrientation(self.humanoid_id)\n",
    "        forward_reward = base_position[0]  # Reward for moving forward\n",
    "        stability_penalty = 0.0  # Add penalties for instability if needed\n",
    "        return forward_reward - stability_penalty\n",
    "    \n",
    "    def _check_done(self):\n",
    "        # Example termination condition: humanoid falls\n",
    "        base_position, _ = p.getBasePositionAndOrientation(self.humanoid_id)\n",
    "        if base_position[2] < 0.5:  # If the humanoid falls below a threshold\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def close(self):\n",
    "        # Disconnect from PyBullet\n",
    "        p.disconnect(self.client)\n",
    "\n",
    "# Instantiate the environment to test its creation\n",
    "env = HumanoidBipedalEnv()\n",
    "obs = env.reset()\n",
    "print(\"Initial Observation:\", obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the RL Agent - Training Script with Stable-Baselines3\n",
    "Implement training code using a suitable RL algorithm (e.g., PPO or SAC) from Stable-Baselines3. Include model instantiation, training loops, checkpoint saving, and logging of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Stable-Baselines3\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import os\n",
    "\n",
    "# Create a vectorized environment for training\n",
    "env = make_vec_env(lambda: HumanoidBipedalEnv(), n_envs=4)\n",
    "\n",
    "# Define the directory to save model checkpoints\n",
    "checkpoint_dir = \"./checkpoints/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define a callback to save model checkpoints\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=checkpoint_dir, name_prefix=\"rl_model\")\n",
    "\n",
    "# Instantiate the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./ppo_humanoid_tensorboard/\")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000, callback=checkpoint_callback)\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"ppo_humanoid_final\")\n",
    "\n",
    "# Load the trained model for evaluation (optional)\n",
    "trained_model = PPO.load(\"ppo_humanoid_final\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "obs = env.reset()\n",
    "for _ in range(1000):  # Run for 1000 steps\n",
    "    action, _states = trained_model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Visualization - Performance Evaluation & Plotting\n",
    "Write evaluation loops to test the trained model in simulation. Use matplotlib to visualize reward progression, state observations, and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for evaluation and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the trained model\n",
    "def evaluate_model(env, model, num_episodes=10):\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards\n",
    "\n",
    "# Run evaluation\n",
    "num_episodes = 10\n",
    "evaluation_rewards = evaluate_model(env, trained_model, num_episodes=num_episodes)\n",
    "\n",
    "# Plot reward progression\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_episodes + 1), evaluation_rewards, marker='o', label=\"Episode Reward\")\n",
    "plt.title(\"Evaluation Rewards Over Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize state observations (example: joint angles over time for one episode)\n",
    "obs = env.reset()\n",
    "joint_angles_over_time = []\n",
    "for _ in range(1000):  # Run for 1000 steps\n",
    "    action, _states = trained_model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    joint_angles = obs[:env.num_joints]  # Extract joint angles from observation\n",
    "    joint_angles_over_time.append(joint_angles)\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "# Convert joint angles to a NumPy array for plotting\n",
    "joint_angles_over_time = np.array(joint_angles_over_time)\n",
    "\n",
    "# Plot joint angles over time\n",
    "plt.figure(figsize=(12, 8))\n",
    "for joint_idx in range(env.num_joints):\n",
    "    plt.plot(joint_angles_over_time[:, joint_idx], label=f\"Joint {joint_idx}\")\n",
    "plt.title(\"Joint Angles Over Time\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Joint Angle (radians)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Integration (Optional) - ROS Integration Skeleton Code\n",
    "Provide skeleton code to facilitate the integration of the simulation model with ROS, including basic setup for handling hardware communication and the simulation-to-real transfer process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ROS-related Python libraries\n",
    "!pip install rospy rospkg\n",
    "\n",
    "# Import necessary libraries for ROS integration\n",
    "import rospy\n",
    "from std_msgs.msg import Float32MultiArray\n",
    "from sensor_msgs.msg import JointState\n",
    "\n",
    "class ROSIntegration:\n",
    "    def __init__(self):\n",
    "        # Initialize the ROS node\n",
    "        rospy.init_node('humanoid_bipedal_ros', anonymous=True)\n",
    "        \n",
    "        # Publisher for sending joint commands\n",
    "        self.joint_command_pub = rospy.Publisher('/humanoid/joint_commands', Float32MultiArray, queue_size=10)\n",
    "        \n",
    "        # Subscriber for receiving joint states\n",
    "        self.joint_state_sub = rospy.Subscriber('/humanoid/joint_states', JointState, self.joint_state_callback)\n",
    "        \n",
    "        # Placeholder for joint states\n",
    "        self.joint_states = None\n",
    "\n",
    "    def joint_state_callback(self, msg):\n",
    "        # Callback to update joint states\n",
    "        self.joint_states = msg\n",
    "\n",
    "    def send_joint_commands(self, commands):\n",
    "        # Publish joint commands to the robot\n",
    "        command_msg = Float32MultiArray()\n",
    "        command_msg.data = commands\n",
    "        self.joint_command_pub.publish(command_msg)\n",
    "\n",
    "    def get_joint_states(self):\n",
    "        # Return the latest joint states\n",
    "        return self.joint_states\n",
    "\n",
    "# Example usage of the ROSIntegration class\n",
    "if __name__ == \"__main__\":\n",
    "    ros_integration = ROSIntegration()\n",
    "    \n",
    "    # Example loop to send commands and read joint states\n",
    "    rate = rospy.Rate(10)  # 10 Hz\n",
    "    while not rospy.is_shutdown():\n",
    "        # Example: Send zero torques to all joints\n",
    "        num_joints = 10  # Replace with the actual number of joints\n",
    "        commands = [0.0] * num_joints\n",
    "        ros_integration.send_joint_commands(commands)\n",
    "        \n",
    "        # Retrieve and print joint states\n",
    "        joint_states = ros_integration.get_joint_states()\n",
    "        if joint_states:\n",
    "            rospy.loginfo(f\"Joint States: {joint_states}\")\n",
    "        \n",
    "        rate.sleep()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
